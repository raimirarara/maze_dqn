{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "maze_dqn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN9rHHyM3m8/7YkEwlkhZRX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raimirarara/maze_dqn/blob/main/maze_dqn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbPikxT4erlV"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# 迷路の読み込み\n",
        "def readMaze():\n",
        "  with open(\"maze.txt\") as f:\n",
        "    maze=f.readlines()\n",
        "  maze=list(map(lambda x: list(x.strip()),maze))\n",
        "  return maze\n",
        "\n",
        "def printMaze(maze):\n",
        "  for i,r in enumerate(maze):\n",
        "    for j,c in enumerate(r):\n",
        "      if i==row and j==col:\n",
        "        print(\"*\", end=\"\")\n",
        "      else:\n",
        "        print(c, end=\"\")\n",
        "    print()\n",
        "\n",
        "\n",
        "def get_s_next(s, a):\n",
        "    global row\n",
        "    global col\n",
        "    global row_next\n",
        "    global col_next\n",
        "    direction = [\"up\", \"right\", \"down\", \"left\"]\n",
        "    next_direction = direction[a]  # 行動aの方向\n",
        "\n",
        "    # 行動から次の状態を決める\n",
        "    if next_direction == \"up\":\n",
        "        row_next = row - 1  # 上に移動するときは状態の数字が3小さくなる\n",
        "        col_next = col\n",
        "        if maze[row_next][col_next] == '#' or maze[row_next][col_next] == 'S': row_next += 1\n",
        "    if next_direction == \"right\":\n",
        "        col_next = col + 1  # 右に移動するときは状態の数字が1大きくなる\n",
        "        row_next = row\n",
        "        if maze[row_next][col_next]== '#' or maze[row_next][col_next] == 'S': col_next -= 1 \n",
        "    if next_direction == \"down\":\n",
        "        row_next = row + 1   # 下に移動するときは状態の数字が3大きくなる\n",
        "        maze[row_next][col_next]\n",
        "        if maze[row_next][col_next] == '#' or maze[row_next][col_next] == 'S': row_next -= 1\n",
        "    if next_direction == \"left\":\n",
        "        col_next = col - 1   # 左に移動するときは状態の数字が1小さくなる\n",
        "        row_next = row_next\n",
        "        if maze[row_next][col_next] == '#' or maze[row_next][col_next] == 'S': col_next += 1\n",
        "        \n",
        "    s_next = maze[row_next][col_next]\n",
        "\n",
        "    return s_next\n",
        "\n",
        "\n",
        "# namedtupleを生成\n",
        "from collections import namedtuple\n",
        "\n",
        "Transition = namedtuple(\n",
        "    'Transition', ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "# 経験を保存するメモリクラスを定義します\n",
        "class ReplayMemory:\n",
        "\n",
        "    def __init__(self, CAPACITY): # 一回だけ読まれる\n",
        "        self.capacity = CAPACITY  # メモリの最大長さ\n",
        "        self.memory = []  # 経験を保存する変数\n",
        "        self.index = 0  # 保存するindexを示す変数\n",
        "\n",
        "    def push(self, state, action, state_next, reward):\n",
        "        '''transition = (state, action, state_next, reward)をメモリに保存する'''\n",
        "\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)  # メモリが満タンでないときは足す\n",
        "\n",
        "        # namedtupleのTransitionを使用し、値とフィールド名をペアにして保存します\n",
        "        self.memory[self.index] = Transition(state, action, state_next, reward)\n",
        "\n",
        "        self.index = (self.index + 1) % self.capacity  # 保存するindexを1つずらす\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        '''batch_size分だけ、ランダムに保存内容を取り出す'''\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        '''関数lenに対して、現在の変数memoryの長さを返す'''\n",
        "        return len(self.memory)\n",
        "\n",
        "# エージェントが持つ脳となるクラスです、DQNを実行します\n",
        "# Q関数をディープラーニングのネットワークをクラスとして定義\n",
        "\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "CAPACITY = 10000\n",
        "\n",
        "\n",
        "class Brain:\n",
        "    def __init__(self, num_states, num_actions):\n",
        "        self.num_actions = num_actions  # 選択しうる行動の数\n",
        "\n",
        "        # 経験を記憶するメモリオブジェクトを生成\n",
        "        self.memory = ReplayMemory(CAPACITY)\n",
        "\n",
        "        # ニューラルネットワークを構築\n",
        "        self.model = nn.Sequential()\n",
        "        self.model.add_module('fc1', nn.Linear(num_states, 32))\n",
        "        self.model.add_module('relu1', nn.ReLU())\n",
        "        self.model.add_module('fc2', nn.Linear(32, 32))\n",
        "        self.model.add_module('relu2', nn.ReLU())\n",
        "        self.model.add_module('fc3', nn.Linear(32, num_actions))\n",
        "\n",
        "        print(self.model)  # ネットワークの形を出力\n",
        "\n",
        "        # 最適化手法の設定\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.0001)\n",
        "\n",
        "    def replay(self):\n",
        "        '''Experience Replayでネットワークの結合パラメータを学習'''\n",
        "\n",
        "        # -----------------------------------------\n",
        "        # 1. メモリサイズの確認\n",
        "        # -----------------------------------------\n",
        "        # 1.1 メモリサイズがミニバッチより小さい間は何もしない\n",
        "        if len(self.memory) < BATCH_SIZE:\n",
        "            return\n",
        "\n",
        "        # -----------------------------------------\n",
        "        # 2. ミニバッチの作成\n",
        "        # -----------------------------------------\n",
        "        # 2.1 メモリからミニバッチ分のデータを取り出す\n",
        "        transitions = self.memory.sample(BATCH_SIZE)\n",
        "\n",
        "        # 2.2 各変数をミニバッチに対応する形に変形\n",
        "        # transitionsは1stepごとの(state, action, state_next, reward)が、BATCH_SIZE分格納されている\n",
        "        # つまり、(state, action, state_next, reward)×BATCH_SIZE\n",
        "        # これをミニバッチにしたい。つまり\n",
        "        # (state×BATCH_SIZE, action×BATCH_SIZE, state_next×BATCH_SIZE, reward×BATCH_SIZE)にする\n",
        "        batch = Transition(*zip(*transitions))\n",
        "\n",
        "        # 2.3 各変数の要素をミニバッチに対応する形に変形し、ネットワークで扱えるようVariableにする\n",
        "        # 例えばstateの場合、[torch.FloatTensor of size 1x4]がBATCH_SIZE分並んでいるのですが、\n",
        "        # それを torch.FloatTensor of size BATCH_SIZEx4 に変換します\n",
        "        # 状態、行動、報酬、non_finalの状態のミニバッチのVariableを作成\n",
        "        # catはConcatenates（結合）のことです。\n",
        "        state_batch = torch.cat(batch.state)\n",
        "        action_batch = torch.cat(batch.action)\n",
        "        reward_batch = torch.cat(batch.reward)\n",
        "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                            if s is not None])\n",
        "\n",
        "        # -----------------------------------------\n",
        "        # 3. 教師信号となるQ(s_t, a_t)値を求める\n",
        "        # -----------------------------------------\n",
        "        # 3.1 ネットワークを推論モードに切り替える\n",
        "        self.model.eval()\n",
        "\n",
        "        # 3.2 ネットワークが出力したQ(s_t, a_t)を求める\n",
        "        # self.model(state_batch)は、右左の両方のQ値を出力しており\n",
        "        # [torch.FloatTensor of size BATCH_SIZEx2]になっている。\n",
        "        # ここから実行したアクションa_tに対応するQ値を求めるため、action_batchで行った行動a_tが右か左かのindexを求め\n",
        "        # それに対応するQ値をgatherでひっぱり出す。\n",
        "        state_action_values = self.model(state_batch).gather(1, action_batch)\n",
        "\n",
        "        # 3.3 max{Q(s_t+1, a)}値を求める。ただし次の状態があるかに注意。\n",
        "\n",
        "        # next_stateがあるかをチェックするインデックスマスクを作成\n",
        "        non_final_mask = torch.ByteTensor(tuple(map(lambda s: s is not None,\n",
        "                                                    batch.next_state)))\n",
        "        # まずは全部0にしておく\n",
        "        next_state_values = torch.zeros(BATCH_SIZE)\n",
        "\n",
        "        # 次の状態があるindexの最大Q値を求める\n",
        "        # 出力にアクセスし、max(1)で列方向の最大値の[値、index]を求めます\n",
        "        # そしてそのQ値（index=0）を出力します\n",
        "        # detachでその値を取り出します\n",
        "        next_state_values[non_final_mask] = self.model(\n",
        "            non_final_next_states).max(1)[0].detach()\n",
        "\n",
        "        # 3.4 教師となるQ(s_t, a_t)値を、Q学習の式から求める\n",
        "        expected_state_action_values = reward_batch + GAMMA * next_state_values\n",
        "\n",
        "        # -----------------------------------------\n",
        "        # 4. 結合パラメータの更新\n",
        "        # -----------------------------------------\n",
        "        # 4.1 ネットワークを訓練モードに切り替える\n",
        "        self.model.train()\n",
        "\n",
        "        # 4.2 損失関数を計算する（smooth_l1_lossはHuberloss）\n",
        "        # expected_state_action_valuesは\n",
        "        # sizeが[minbatch]になっているので、unsqueezeで[minibatch x 1]へ\n",
        "        loss = F.smooth_l1_loss(state_action_values,\n",
        "                                expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "        # 4.3 結合パラメータを更新する\n",
        "        self.optimizer.zero_grad()  # 勾配をリセット\n",
        "        loss.backward()  # バックプロパゲーションを計算\n",
        "        self.optimizer.step()  # 結合パラメータを更新\n",
        "\n",
        "    def decide_action(self, state, episode):\n",
        "        '''現在の状態に応じて、行動を決定する'''\n",
        "        # ε-greedy法で徐々に最適行動のみを採用する\n",
        "        epsilon = 0.5 * (1 / (episode + 1))\n",
        "\n",
        "        if epsilon <= np.random.uniform(0, 1):\n",
        "            self.model.eval()  # ネットワークを推論モードに切り替える\n",
        "            with torch.no_grad():\n",
        "                action = self.model(state).max(1)[1].view(1, 1)\n",
        "            # ネットワークの出力の最大値のindexを取り出します = max(1)[1]\n",
        "            # .view(1,1)は[torch.LongTensor of size 1]　を size 1x1 に変換します\n",
        "\n",
        "        else:\n",
        "            # 0,1, 2, 3の行動をランダムに返す\n",
        "            action = torch.LongTensor(\n",
        "                [[random.randrange(self.num_actions)]])  # 0, 1, 2, 3の行動をランダムに返す\n",
        "            # actionは[torch.LongTensor of size 1x1]の形になります\n",
        "\n",
        "        return action\n",
        "\n",
        "\n",
        "\n",
        "maze = readMaze()\n",
        "turn = 1\n",
        "row = 1 # 現在の自分のいる列\n",
        "col = 1 # 現在の自分のいる行\n",
        "row_next = 1\n",
        "col_next = 1\n",
        "\n",
        "step_history = [] # Goalするのにかかったstep数を記録するリスト\n",
        "\n",
        "# Q学習で迷路を解く\n",
        "eta = 0.1  # 学習率\n",
        "GAMMA = 0.9  # 時間割引率\n",
        "episode = 1\n",
        "\n",
        "MAX_STEPS = 500  # 1試行のstep数\n",
        "NUM_EPISODES = 100  # 最大試行回数\n",
        "\n",
        "num_states = 2\n",
        "num_actions = 4\n",
        "\n",
        "brain = Brain(num_states, num_actions)\n",
        "\n",
        "for episode in range(NUM_EPISODES):  \n",
        "    print(\"エピソード:\" + str(episode))\n",
        "\n",
        "    # Q学習で迷路を解き、移動した履歴と更新したQを求める\n",
        "    global row\n",
        "    global col\n",
        "    global row_next\n",
        "    global col_next\n",
        "    global s_num\n",
        "    global s_num_next\n",
        "    row = 1 # スタート地点の座標を入力\n",
        "    col = 1\n",
        "    row_next = 1\n",
        "    col_next = 1\n",
        "    s = s_next = maze[row][col] #スタート地点\n",
        "    s_num = np.array([row, col])\n",
        "    state = torch.from_numpy(s_num).type(torch.FloatTensor)\n",
        "    state = torch.unsqueeze(state, 0)\n",
        "    s_a_history = [[0, np.nan]]  # エージェントの移動を記録するリスト\n",
        "\n",
        "    for step in range(MAX_STEPS):  # MAX_STEPSまでループ\n",
        "        a = a_next = brain.decide_action(state, episode) # 行動更新\n",
        "\n",
        "        s_a_history[-1][1] = a\n",
        "        # 現在の状態（つまり一番最後なのでindex=-1）に行動を代入\n",
        "\n",
        "        s_next = get_s_next(s, a) # s_nextを更新\n",
        "        s_num = np.array([row, col]) # s_numを更新\n",
        "        state = torch.from_numpy(s_num).type(torch.FloatTensor) # s_numをテンソルに変換\n",
        "        state = torch.unsqueeze(state, 0)\n",
        "        s_num_next = np.array([ row_next, col_next])\n",
        "        state_next = torch.from_numpy(s_num_next).type(torch.FloatTensor)\n",
        "        state_next = torch.unsqueeze(state_next, 0)\n",
        "\n",
        "          # 報酬を与え,　次の行動を求めます\n",
        "        if s_next == 'G':\n",
        "            a_next = np.nan\n",
        "            state_next = None\n",
        "            reward = torch.FloatTensor([1.0]) # 報酬を与える\n",
        "        else:\n",
        "            reward = torch.FloatTensor([0.0]) # 普段は報酬0\n",
        "            a_next = brain.decide_action(state, episode)\n",
        "            # 次の行動a_nextを求めます。\n",
        "\n",
        "\n",
        "        # メモリに経験を追加\n",
        "        brain.memory.push(state, a, state_next, reward)\n",
        "\n",
        "            \n",
        "        s_a_history.append([s_next, np.nan])\n",
        "        # 次の状態を代入。行動はまだ分からないのでnanにしておく\n",
        "        print(\"step = \" , step)\n",
        "        printMaze(maze)\n",
        "\n",
        "        # Experience ReplayでQ関数を更新する\n",
        "        brain.replay()\n",
        "\n",
        "        # 終了判定\n",
        "        if s_next == 'G':  # ゴール地点なら終了\n",
        "            break\n",
        "        else:\n",
        "            row = row_next\n",
        "            col = col_next\n",
        "            s = s_next\n",
        "            state = state_next\n",
        "            s_num = s_num_next\n",
        "\n",
        "    # print(s_a_history)\n",
        "    print(\"迷路を解くのにかかったステップ数は\" + str(len(s_a_history) - 1) + \"です\")\n",
        "    step_history.append(len(s_a_history) - 1)\n",
        "    # print(step_history)\n",
        "\n",
        "\n",
        "# グラフを描画\n",
        "plt.title(\"learningprocess\")\n",
        "plt.xlabel(\"num_episode\")\n",
        "plt.ylabel(\"num_step\")\n",
        "x = range(NUM_EPISODES)\n",
        "y = step_history\n",
        "\n",
        "plt.plot(x, y)\n",
        "plt.show"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}