{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "maze_dqn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPceDCzD9gP/6xLRB40lvq5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raimirarara/maze_dqn/blob/main/maze_dqn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbPikxT4erlV"
      },
      "source": [
        "\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# 迷路を作成する\n",
        "\n",
        "import os\n",
        "\n",
        "# 迷路を作成する\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "width = 9 # 迷路の幅を決定する\n",
        "height = 7 # 迷路の高さを決定する\n",
        "cell = []\n",
        "start_cells = [] # 壁を作り始める座標のリスト\n",
        "start_pos = []\n",
        "goal_pos = []\n",
        "\n",
        "def extend_wall(x ,y):\n",
        "  #  ここで伸ばすことができる方向を取得する(1マス先が通路で2マス先まで範囲内)\n",
        "  #  2マス先が壁で自分自身の場合、伸ばせない\n",
        "  directions = [] # 壁を伸ばす方向のリスト\n",
        "  if (maze[x, y - 1] == ' ' and maze[x, y - 2] != '#'):\n",
        "      directions.append(\"direction_up\");\n",
        "  if (maze[x + 1, y] == ' ' and maze[x + 2, y] != '#'):\n",
        "      directions.append(\"direction_right\");\n",
        "  if (maze[x, y + 1] == ' ' and maze[x, y + 2] != '#'):\n",
        "      directions.append(\"direction_down\");\n",
        "  if (maze[x - 1, y] == ' ' and maze[x - 2, y] != '#'):\n",
        "      directions.append(\"direction_left\");\n",
        "\n",
        "  for i in range(len(directions)): \n",
        "    set_wall(x, y) # 現在いる場所に#を入れる\n",
        "    is_path = False\n",
        "    selected_dir = random.choice(directions)\n",
        "    if selected_dir == \"direction_up\":\n",
        "      y -= 1 # 一個上に#入れる\n",
        "      set_wall(x, y)\n",
        "      y -= 1 # さらに一個上に#を入れる\n",
        "      set_wall(x, y)\n",
        "    if selected_dir == \"direction_right\":\n",
        "      x += 1\n",
        "      set_wall(x, y)\n",
        "      x += 1\n",
        "      set_wall(x, y)\n",
        "    if selected_dir == \"direction_down\":\n",
        "      y += 1\n",
        "      set_wall(x, y)\n",
        "      y += 1\n",
        "      set_wall(x, y)\n",
        "    if selected_dir == \"direction_left\":\n",
        "      x -= 1\n",
        "      set_wall(x, y)\n",
        "      x -= 1\n",
        "      set_wall(x, y)\n",
        "    \n",
        "def set_wall(x, y):\n",
        "  maze[x][y] = '#'\n",
        "\n",
        "def set_start():\n",
        "  _index = []\n",
        "  _index = random.choice(start_pos)\n",
        "  maze[_index[0], _index[1]] = 'S'\n",
        "  # print(_index) Sの座標\n",
        "  start_col = _index[0]\n",
        "  start_row = _index[1]\n",
        "  return start_col, start_row\n",
        "\n",
        "def set_goal():\n",
        "  _index = []\n",
        "  _index = random.choice(goal_pos)\n",
        "  maze[_index[0], _index[1]] = 'G'\n",
        "  # print(_index) Gの座標\n",
        "\n",
        "\n",
        "maze = np.zeros(width*height , str).reshape(width, height)\n",
        "\n",
        "for y in range(height):\n",
        "  for x in range(width):\n",
        "    if (x == 0 or y == 0 or x == width-1 or y == height-1):\n",
        "      maze[x][y] = '#'\n",
        "    else:\n",
        "      maze[x][y] = ' '\n",
        "      if x % 2 == 0 and y % 2 == 0:\n",
        "        start_cells.append([x, y])\n",
        "\n",
        "for i in range(len(start_cells)):\n",
        "  index = random.choice(start_cells)\n",
        "  start_cells.remove(index)\n",
        "  cell_x = index[0]\n",
        "  cell_y = index[1]\n",
        "\n",
        "  if maze[cell_x][cell_y] == ' ':\n",
        "    extend_wall(cell_x, cell_y)\n",
        "\n",
        "for y in range(int(height/2)):\n",
        "  for x in range(int(width/2)):\n",
        "    if maze[x][y] == \" \":\n",
        "      start_pos.append([x, y])\n",
        "\n",
        "for y in range(height - int(height/2), height):\n",
        "  for x in range(width - int(width/2), width):\n",
        "    if maze[x][y] == \" \":\n",
        "      goal_pos.append([x, y])\n",
        "\n",
        "start_col, start_row = set_start()\n",
        "set_goal()\n",
        "\n",
        "np.savetxt(\"maze.txt\", maze, fmt=\"%s\", delimiter='') # delimiter=' ': デフォルトで'スペースを開けるようになっているので注意\n",
        "\n",
        "# 迷路の読み込み\n",
        "def readMaze():\n",
        "  with open(\"maze.txt\") as f:\n",
        "    maze=f.readlines()\n",
        "  maze=list(map(lambda x: list(x.strip()),maze))\n",
        "  return maze\n",
        "\n",
        "# 迷路を表示\n",
        "def printMaze(maze):\n",
        "  for i,r in enumerate(maze):\n",
        "    for j,c in enumerate(r):\n",
        "      if i==row and j==col: #  row(行), col(列)を現在の位置とする\n",
        "        print(\"*\", end=\"\")\n",
        "      else:\n",
        "        print(c, end=\"\")\n",
        "    print()\n",
        "\n",
        "\n",
        "def get_s_next(s, a , row, col):\n",
        "    direction = [\"up\", \"right\", \"down\", \"left\"]\n",
        "    next_direction = direction[a]  # 行動aの方向\n",
        "\n",
        "    # 行動から次の状態を決める\n",
        "    if next_direction == \"up\":\n",
        "      if maze[row - 1][col] != '#':\n",
        "        row -= 1\n",
        "    if next_direction == \"right\":\n",
        "      if maze[row][col + 1] != '#':\n",
        "        col += 1\n",
        "    if next_direction == \"down\":\n",
        "       if maze[row + 1][col] != '#':\n",
        "        row += 1\n",
        "    if next_direction == \"left\":\n",
        "       if maze[row][col - 1] != '#':\n",
        "        col -= 1\n",
        "\n",
        "    s_next = maze[row][col]\n",
        "\n",
        "    return s_next, row, col\n",
        "\n",
        "\n",
        "# namedtupleを生成\n",
        "from collections import namedtuple\n",
        "\n",
        "Transition = namedtuple(\n",
        "    'Transition', ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "# 経験を保存するメモリクラスを定義します\n",
        "class ReplayMemory:\n",
        "\n",
        "    def __init__(self, CAPACITY): # 一回だけ読まれる\n",
        "        self.capacity = CAPACITY  # メモリの最大長さ\n",
        "        self.memory = []  # 経験を保存する変数\n",
        "        self.index = 0  # 保存するindexを示す変数\n",
        "\n",
        "    def push(self, state, action, state_next, reward):\n",
        "        '''transition = (state, action, state_next, reward)をメモリに保存する'''\n",
        "\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)  # メモリが満タンでないときは足す\n",
        "\n",
        "        # namedtupleのTransitionを使用し、値とフィールド名をペアにして保存します\n",
        "        self.memory[self.index] = Transition(state, action, state_next, reward)\n",
        "\n",
        "        self.index = (self.index + 1) % self.capacity  # 保存するindexを1つずらす\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        '''batch_size分だけ、ランダムに保存内容を取り出す'''\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        '''関数lenに対して、現在の変数memoryの長さを返す'''\n",
        "        return len(self.memory)\n",
        "\n",
        "# エージェントが持つ脳となるクラスです、DQNを実行します\n",
        "# Q関数をディープラーニングのネットワークをクラスとして定義\n",
        "\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "CAPACITY = 10000\n",
        "\n",
        "\n",
        "class Brain:\n",
        "    def __init__(self, num_states, num_actions):\n",
        "        self.num_actions = num_actions  # 選択しうる行動の数\n",
        "\n",
        "        # 経験を記憶するメモリオブジェクトを生成\n",
        "        self.memory = ReplayMemory(CAPACITY)\n",
        "\n",
        "        # ニューラルネットワークを構築\n",
        "        self.model = nn.Sequential()\n",
        "        self.model.add_module('fc1', nn.Linear(num_states, 32))\n",
        "        self.model.add_module('relu1', nn.ReLU())\n",
        "        self.model.add_module('fc2', nn.Linear(32, 32))\n",
        "        self.model.add_module('relu2', nn.ReLU())\n",
        "        self.model.add_module('fc3', nn.Linear(32, num_actions))\n",
        "\n",
        "        print(self.model)  # ネットワークの形を出力\n",
        "\n",
        "        # 最適化手法の設定\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.0001)\n",
        "\n",
        "    def replay(self):\n",
        "        '''Experience Replayでネットワークの結合パラメータを学習'''\n",
        "\n",
        "        # -----------------------------------------\n",
        "        # 1. メモリサイズの確認\n",
        "        # -----------------------------------------\n",
        "        # 1.1 メモリサイズがミニバッチより小さい間は何もしない\n",
        "        if len(self.memory) < BATCH_SIZE:\n",
        "            return\n",
        "\n",
        "        # -----------------------------------------\n",
        "        # 2. ミニバッチの作成\n",
        "        # -----------------------------------------\n",
        "        # 2.1 メモリからミニバッチ分のデータを取り出す\n",
        "        transitions = self.memory.sample(BATCH_SIZE)\n",
        "\n",
        "        # 2.2 各変数をミニバッチに対応する形に変形\n",
        "        # transitionsは1stepごとの(state, action, state_next, reward)が、BATCH_SIZE分格納されている\n",
        "        # つまり、(state, action, state_next, reward)×BATCH_SIZE\n",
        "        # これをミニバッチにしたい。つまり\n",
        "        # (state×BATCH_SIZE, action×BATCH_SIZE, state_next×BATCH_SIZE, reward×BATCH_SIZE)にする\n",
        "        batch = Transition(*zip(*transitions))\n",
        "\n",
        "        # 2.3 各変数の要素をミニバッチに対応する形に変形し、ネットワークで扱えるようVariableにする\n",
        "        # 例えばstateの場合、[torch.FloatTensor of size 1x4]がBATCH_SIZE分並んでいるのですが、\n",
        "        # それを torch.FloatTensor of size BATCH_SIZEx4 に変換します\n",
        "        # 状態、行動、報酬、non_finalの状態のミニバッチのVariableを作成\n",
        "        # catはConcatenates（結合）のことです。\n",
        "        state_batch = torch.cat(batch.state)\n",
        "        action_batch = torch.cat(batch.action)\n",
        "        reward_batch = torch.cat(batch.reward)\n",
        "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                            if s is not None])\n",
        "\n",
        "        # -----------------------------------------\n",
        "        # 3. 教師信号となるQ(s_t, a_t)値を求める\n",
        "        # -----------------------------------------\n",
        "        # 3.1 ネットワークを推論モードに切り替える\n",
        "        self.model.eval()\n",
        "\n",
        "        # 3.2 ネットワークが出力したQ(s_t, a_t)を求める\n",
        "        # self.model(state_batch)は、右左の両方のQ値を出力しており\n",
        "        # [torch.FloatTensor of size BATCH_SIZEx2]になっている。\n",
        "        # ここから実行したアクションa_tに対応するQ値を求めるため、action_batchで行った行動a_tが右か左かのindexを求め\n",
        "        # それに対応するQ値をgatherでひっぱり出す。\n",
        "        state_action_values = self.model(state_batch).gather(1, action_batch)\n",
        "\n",
        "        # 3.3 max{Q(s_t+1, a)}値を求める。ただし次の状態があるかに注意。\n",
        "\n",
        "        # next_stateがあるかをチェックするインデックスマスクを作成\n",
        "        non_final_mask = torch.ByteTensor(tuple(map(lambda s: s is not None,\n",
        "                                                    batch.next_state)))\n",
        "        # まずは全部0にしておく\n",
        "        next_state_values = torch.zeros(BATCH_SIZE)\n",
        "\n",
        "        # 次の状態があるindexの最大Q値を求める\n",
        "        # 出力にアクセスし、max(1)で列方向の最大値の[値、index]を求めます\n",
        "        # そしてそのQ値（index=0）を出力します\n",
        "        # detachでその値を取り出します\n",
        "        next_state_values[non_final_mask] = self.model(\n",
        "            non_final_next_states).max(1)[0].detach()\n",
        "\n",
        "        # 3.4 教師となるQ(s_t, a_t)値を、Q学習の式から求める\n",
        "        expected_state_action_values = reward_batch + GAMMA * next_state_values\n",
        "\n",
        "        # -----------------------------------------\n",
        "        # 4. 結合パラメータの更新\n",
        "        # -----------------------------------------\n",
        "        # 4.1 ネットワークを訓練モードに切り替える\n",
        "        self.model.train()\n",
        "\n",
        "        # 4.2 損失関数を計算する（smooth_l1_lossはHuberloss）\n",
        "        # expected_state_action_valuesは\n",
        "        # sizeが[minbatch]になっているので、unsqueezeで[minibatch x 1]へ\n",
        "        loss = F.smooth_l1_loss(state_action_values,\n",
        "                                expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "        # 4.3 結合パラメータを更新する\n",
        "        self.optimizer.zero_grad()  # 勾配をリセット\n",
        "        loss.backward()  # バックプロパゲーションを計算\n",
        "        self.optimizer.step()  # 結合パラメータを更新\n",
        "\n",
        "    def decide_action(self, state, episode):\n",
        "        '''現在の状態に応じて、行動を決定する'''\n",
        "        # ε-greedy法で徐々に最適行動のみを採用する\n",
        "        epsilon = 0.5 * (1 / (episode + 1))\n",
        "\n",
        "        if epsilon <= np.random.uniform(0, 1):\n",
        "            self.model.eval()  # ネットワークを推論モードに切り替える\n",
        "            with torch.no_grad():\n",
        "                action = self.model(state).max(1)[1].view(1, 1)\n",
        "            # ネットワークの出力の最大値のindexを取り出します = max(1)[1]\n",
        "            # .view(1,1)は[torch.LongTensor of size 1]　を size 1x1 に変換します\n",
        "\n",
        "        else:\n",
        "            # 0,1, 2, 3の行動をランダムに返す\n",
        "            action = torch.LongTensor(\n",
        "                [[random.randrange(self.num_actions)]])  # 0, 1, 2, 3の行動をランダムに返す\n",
        "            # actionは[torch.LongTensor of size 1x1]の形になります\n",
        "\n",
        "        return action\n",
        "\n",
        "\n",
        "\n",
        "maze = readMaze()\n",
        "turn = 1\n",
        "\n",
        "step_history = [] # Goalするのにかかったstep数を記録するリスト\n",
        "\n",
        "# Q学習で迷路を解く\n",
        "eta = 0.1  # 学習率\n",
        "GAMMA = 0.9  # 時間割引率\n",
        "episode = 1\n",
        "\n",
        "MAX_STEPS = 500  # 1試行のstep数\n",
        "NUM_EPISODES = 100  # 最大試行回数\n",
        "\n",
        "num_states = 2\n",
        "num_actions = 4\n",
        "\n",
        "brain = Brain(num_states, num_actions)\n",
        "\n",
        "for episode in range(NUM_EPISODES):  \n",
        "    print(\"エピソード:\" + str(episode))\n",
        "\n",
        "    # row, colをスタート地点に戻す\n",
        "    row = start_row \n",
        "    col = start_col\n",
        "    s = maze[row][col] # スタート地点 sは, \" \", \"#\", \"S\", \"G\"の値をとる\n",
        "    s_a_history = [[0, np.nan]]  # エージェントの移動を記録するリスト\n",
        "\n",
        "    for step in range(MAX_STEPS):  # MAX_STEPSまでループ\n",
        "\n",
        "        s_array = np.array([row, col]) # 現在いる位置の座標を配列で表示(次にテンソルに変換するため)\n",
        "        state = torch.from_numpy(s_array).type(torch.FloatTensor) # s_arrayをテンソルに変換\n",
        "        state = torch.unsqueeze(state, 0)\n",
        "\n",
        "        a = a_next = brain.decide_action(state, episode) # 行動更新\n",
        "\n",
        "        s_a_history[-1][1] = a\n",
        "        # 現在の状態（つまり一番最後なのでindex=-1）に行動を代入\n",
        "\n",
        "        # row, colが更新されて次の状態(\" \", \"#\", \"S\", \"G\")がs_nextに入る\n",
        "        s_next, next_row, next_col = get_s_next(s, a, row, col)\n",
        "\n",
        "        s_array_next = np.array([next_row, next_col])\n",
        "        state_next = torch.from_numpy(s_array_next).type(torch.FloatTensor)\n",
        "        state_next = torch.unsqueeze(state_next, 0)\n",
        "\n",
        "          # 報酬を与え,　次の行動を求めます\n",
        "        if s_next == 'G':\n",
        "            a_next = np.nan\n",
        "            state_next = None\n",
        "            reward = torch.FloatTensor([1.0]) # 報酬を与える\n",
        "        else:\n",
        "            reward = torch.FloatTensor([0.0]) # 普段は報酬0\n",
        "            a_next = brain.decide_action(state, episode)\n",
        "            # 次の行動a_nextを求めます。\n",
        "\n",
        "\n",
        "        # メモリに経験を追加\n",
        "        brain.memory.push(state, a, state_next, reward)\n",
        "\n",
        "            \n",
        "        s_a_history.append([s_next, np.nan])\n",
        "        # 次の状態を代入。行動はまだ分からないのでnanにしておく\n",
        "\n",
        "        print(\"step = \" , step) # step数を表示\n",
        "        printMaze(maze) # 迷路を表示\n",
        "\n",
        "        # Experience ReplayでQ関数を更新する\n",
        "        brain.replay()\n",
        "\n",
        "        # 終了判定\n",
        "        if s_next == 'G':  # ゴール地点なら終了\n",
        "            break\n",
        "        else:\n",
        "            s = s_next\n",
        "            state = state_next\n",
        "            s_array = s_array_next\n",
        "            row = next_row\n",
        "            col = next_col\n",
        "\n",
        "    # print(s_a_history)\n",
        "    print(\"迷路を解くのにかかったステップ数は\" + str(len(s_a_history) - 1) + \"です\")\n",
        "    step_history.append(len(s_a_history) - 1)\n",
        "    # print(step_history)\n",
        "\n",
        "\n",
        "# グラフを描画\n",
        "plt.title(\"learningprocess\")\n",
        "plt.xlabel(\"num_episode\")\n",
        "plt.ylabel(\"num_step\")\n",
        "x = range(NUM_EPISODES)\n",
        "y = step_history\n",
        "\n",
        "plt.plot(x, y)\n",
        "plt.show"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}